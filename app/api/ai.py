import subprocess
from fastapi import APIRouter, File, UploadFile
from fastapi.responses import JSONResponse

from app.schema.ai_schema import PptExtractResponse, YouTubeURLRequest, AudioTranscibeResponse
from app.service.ai_service import clova_segmentation, download_youtube_audio, generate_note, get_libreoffice_path, analyze_image, process_important_segments, transcribe_audio_file, extract_ppt_text  # ì„œë¹„ìŠ¤ ëª¨ë“ˆ ì„í¬íŠ¸

from app.service.mapping_service import LectureSlideMapper
from app.schema.mapping_schema import LectureTextRequest, MappingResultResponse

from collections import OrderedDict
from pptx import Presentation
from pdf2image import convert_from_path
from fastapi import Form
import tempfile
import os
import shutil
import base64
import io
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

router = APIRouter()

@router.post("/extract-youtube-audio")
async def youtube_audio(request: YouTubeURLRequest):
    """
    YouTube ì˜ìƒìœ¼ë¡œë¶€í„° ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œ 
    """
    try:
        file_path = download_youtube_audio(request.youtube_url)
        return {"message": "YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ", "data": file_path}
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
        return JSONResponse(status_code=500, content={"message": f"ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}", "data": None})
    

@router.post("/transcribe-audio", response_model=AudioTranscibeResponse)
async def transcribe_audio(audio_file: UploadFile = File(...)):
    """
    ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisper ëª¨ë¸ë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    try:
        transcribed_text = transcribe_audio_file(audio_file)
        return {"message": "ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ ë³€í™˜ ì„±ê³µ", "data": transcribed_text}
    except Exception as e:
        return JSONResponse(status_code=500, content={"message": f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}", "data": None})
    

@router.post("/extract-ppt-text", response_model=PptExtractResponse)
def extract_ppt_text_endpoint(ppt_file: UploadFile = File(...)):
    """
    ì—…ë¡œë“œëœ PPTX íŒŒì¼ì—ì„œ ìŠ¬ë¼ì´ë“œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ 
    """
    try:
        text_list = extract_ppt_text(ppt_file)
        return {"message": "PPT í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ", "data": text_list}
    except ValueError as e:
        # ì˜ëª»ëœ ì…ë ¥ ë“±ìœ¼ë¡œ ì¸í•œ ì‚¬ìš©ì ì˜¤ë¥˜
        return JSONResponse(status_code=400, content={"message": str(e), "data": None})
    except Exception as e:
        return JSONResponse(status_code=500, content={"message": f"PPT ì²˜ë¦¬ ì‹¤íŒ¨: {e}", "data": None})


mapper = LectureSlideMapper()
@router.post("/text-slide-mapping")
def map_lecture_to_slide(request: LectureTextRequest):
    """
    ê°•ì˜ í…ìŠ¤íŠ¸ + ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë§¤í•‘ í›„ ìŠ¬ë¼ì´ë“œë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ë£¹í™”
    (segment_index, text, similarity_score ëª¨ë‘ í¬í•¨)
    """

    # ê°•ì˜ í…ìŠ¤íŠ¸ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬ (í•˜ë‚˜ì˜ ì„¸ê·¸ë¨¼íŠ¸ì— 10ë¬¸ì¥ ê³ ì •)
    segments = mapper.preprocess_and_split_text(request.lecture_text)

    results = mapper.map_lecture_text_to_slides(
        segment_texts=segments, 
        slide_texts=request.slide_texts
    )

    print("ê°•ì˜ ë…¹ìŒë³¸ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì™„ë£Œ")

    # 2. ìŠ¬ë¼ì´ë“œë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ê¸°
    slide_to_segments = {}

    for res in results:
        segment_idx = res["segment_index"]     
        slide_idx = res["matched_slide_index"]  
        similarity_score = res["similarity_score"] 
        slide_key = f"slide{slide_idx}"

        if slide_key not in slide_to_segments:
            slide_to_segments[slide_key] = []

        # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€
        slide_to_segments[slide_key].append({
            "segment_index": segment_idx,
            "text": segments[segment_idx],
            "similarity_score": round(similarity_score, 4)  # ì†Œìˆ˜ì  4ìë¦¬ë¡œ ê¹”ë”í•˜ê²Œ
        })

    print("ìŠ¬ë¼ì´ë“œë³„ ì„¸ê·¸ë¨¼íŠ¸ ë§¤ì¹­ ì™„ë£Œ")

    # 3. ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return JSONResponse(content=slide_to_segments)


LIBREOFFICE_PATH = get_libreoffice_path()
@router.post("/image-captioning")
async def image_captioning(file: UploadFile = File(...)):
    """
    PPT -> PDF -> Image -> ìŠ¬ë¼ì´ë“œë³„ ì„¤ëª… ì¶œë ¥ 
    """

    try:
        # 1. ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp()

        # 2. ì—…ë¡œë“œëœ PPTX íŒŒì¼ ì €ì¥
        ppt_path = os.path.join(temp_dir, file.filename)
        with open(ppt_path, "wb") as f:
            shutil.copyfileobj(file.file, f)

        print("PPT íŒŒì¼ ì €ì¥ ì™„ë£Œ")

        # 3. PPT íŒŒì¼ ì—´ì–´ì„œ ìŠ¬ë¼ì´ë“œ ìˆ˜ í™•ì¸
        prs = Presentation(ppt_path)
        total_slides = len(prs.slides)

        # 4. PPTX -> PDF ë³€í™˜
        subprocess.run([LIBREOFFICE_PATH, "--headless", "--convert-to", "pdf", "--outdir", temp_dir, ppt_path], check=True)

        # ë³€í™˜ëœ PDF íŒŒì¼ ê²½ë¡œ
        ppt_filename = os.path.splitext(file.filename)[0] + ".pdf"
        pdf_path = os.path.join(temp_dir, ppt_filename)
        if not os.path.exists(pdf_path):
            raise Exception("PDF ë³€í™˜ ì‹¤íŒ¨")
        
        print("PDF ë³€í™˜ ì™„ë£Œ")
        
        # 5. PDF -> ì´ë¯¸ì§€ ë³€í™˜
        images = convert_from_path(
            pdf_path,
            poppler_path=r"C:\Program Files\Poppler\poppler-24.08.0\Library\bin"  # ğŸ‘‰ poppler ê²½ë¡œ ìˆ˜ì •í•  ê²ƒ
        )
        if not images:
            print(f"ìŠ¬ë¼ì´ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        print("PDF - ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ")

        # 6. ëª¨ë“  ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
        results = {
            "total_slide": total_slides
        }

        for idx, img in enumerate(images, start=1):
            # PIL Image ê°ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()

            # base64ë¥¼ OpenAI Vision ëª¨ë¸ë¡œ ì „ì†¡
            image_url = f"data:image/jpeg;base64,{img_base64}"
            caption = await analyze_image(image_url)

            # ìŠ¬ë¼ì´ë“œ ê²°ê³¼ ì €ì¥
            results[f"slide{idx}"] = caption
        
        print("ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì™„ë£Œ")


        # 7. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        shutil.rmtree(temp_dir)

        return JSONResponse(content=results)

    except Exception as e:
        logger.debug(e, stack_info=True)
        # ì„ì‹œíŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        return JSONResponse(status_code=500, content={"message": f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"})



mapper = LectureSlideMapper()
LIBREOFFICE_PATH = get_libreoffice_path()
@router.post("/process-lecture")
async def process_lecture(
    audio_file: UploadFile = File(...),
    ppt_file: UploadFile = File(...),
    skip_transcription: bool = Form(False)  # ğŸ”¥ ì˜µì…˜ ì¶”ê°€: ê¸°ë³¸ì€ False
):
    """
    ê°•ì˜ ë…¹ìŒë³¸ + PPT íŒŒì¼ ì…ë ¥ë°›ì•„ì„œ ìŠ¬ë¼ì´ë“œë³„ ì„¸ê·¸ë¨¼íŠ¸ ë§¤ì¹­ ê²°ê³¼ 
    """

    try:
        ### 1. ì˜¤ë””ì˜¤ íŒŒì¼ í…ìŠ¤íŠ¸ ë³€í™˜ (ë˜ëŠ” ìŠ¤í‚µ)
        if skip_transcription:
            # ğŸ”¥ ë³€í™˜ ìŠ¤í‚µ: ì´ë¯¸ ì €ì¥ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
            with open(os.path.join("download", "lecture_text.txt"), "r", encoding="utf-8") as f:
                lecture_text = f.read()
        else:
            # ğŸ”¥ ë³€í™˜ ìˆ˜í–‰
            lecture_text = transcribe_audio_file(audio_file)
        
        print("ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ")

        # 2. PPT íŒŒì¼ ì €ì¥ ë° ë³€í™˜
        temp_dir = tempfile.mkdtemp()
        ppt_path = os.path.join(temp_dir, ppt_file.filename)

        with open(ppt_path, "wb") as f:
            shutil.copyfileobj(ppt_file.file, f)

        subprocess.run([LIBREOFFICE_PATH, "--headless", "--convert-to", "pdf", "--outdir", temp_dir, ppt_path], check=True)
        pdf_filename = os.path.splitext(ppt_file.filename)[0] + ".pdf"
        pdf_path = os.path.join(temp_dir, pdf_filename)

        if not os.path.exists(pdf_path):
            raise Exception("PDF ë³€í™˜ ì‹¤íŒ¨")

        images = convert_from_path(
            pdf_path,
            poppler_path=r"C:\Program Files\Poppler\poppler-24.08.0\Library\bin"
        )

        ### 3. ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ë³„ ìº¡ì…˜ ì¶”ì¶œ
        slide_captions = []
        for img in images:
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            image_url = f"data:image/jpeg;base64,{img_base64}"

            caption = await analyze_image(image_url)
            slide_captions.append(caption)
        
        print("ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì™„ë£Œ")

        # 4. CLOVA Segmentation ì‚¬ìš©í•´ì„œ ê°•ì˜ ë…¹ìŒë³¸ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ê¸° / ê³ ì •ëœ ê°œìˆ˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ìš©
        # segments = clova_segmentation(lecture_text)
        segments = mapper.preprocess_and_split_text(lecture_text) 

        print("Clova API í™œìš© ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì™„ë£Œ")

        # 5. ì„¸ê·¸ë¨¼íŠ¸-ìŠ¬ë¼ì´ë“œ ë§¤í•‘
        results = mapper.map_lecture_text_to_slides(
            segment_texts=segments,
            slide_texts=slide_captions
        )

        print("ì„¸ê·¸ë¨¼íŠ¸ ìŠ¬ë¼ì´ë“œ ë§¤í•‘ ì™„ë£Œ")

        # ì¤‘ìš” ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ 
        raw_json = {f"segment{i}": seg for i, seg in enumerate(segments)}
        important_segments_result = process_important_segments(raw_json)

        print("ë§¤í•‘ ê²°ê³¼ ìŠ¬ë¼ì´ë“œ ë³„ë¡œ ë¬¶ê¸°")

        # 6. ë§¤í•‘ ê²°ê³¼ë¥¼ ìŠ¬ë¼ì´ë“œë³„ë¡œ ë¬¶ê¸°
        slide_to_segments = {}
        for slide_idx in range(len(slide_captions)):
            slide_key = f"slide{slide_idx + 1}"
            slide_to_segments[slide_key] = []

        for res in results:
            segment_idx = res["segment_index"]
            slide_idx = res["matched_slide_index"]
            similarity_score = res["similarity_score"]
            slide_key = f"slide{slide_idx+1}"

            seg_id = f"segment{segment_idx}"
            is_important = seg_id in important_segments_result

            slide_to_segments[slide_key].append({
                "segment_key": seg_id,
                "text": segments[segment_idx],
                "isImportant": str(is_important).lower(),
                "reason": important_segments_result.get(seg_id, {}).get("reason", ""),
                "linkedConcept": "",
                "pageNumber": ""
            })

        sorted_slide_to_segments = OrderedDict(sorted(slide_to_segments.items(), key=lambda x: int(x[0].replace("slide", ""))))

        print("ê° ìŠ¬ë¼ì´ë“œë³„ í•„ê¸° ìƒì„± ì‹œì‘")

        final_notes = {}
        for slide_key, segment_list in sorted_slide_to_segments.items():
            slide_idx = int(slide_key.replace("slide", "")) - 1
            slide_caption = slide_captions[slide_idx]
            matched_segment_texts = [seg["text"] for seg in segment_list]
            note_sections, cost = generate_note(slide_caption, matched_segment_texts)
            note_sections["Matched Segments"] = { 
                seg["segment_key"]: {
                    "text": seg["text"],
                    "isImportant": seg["isImportant"],
                    "reason": seg["reason"],
                    "linkedConcept": seg["linkedConcept"],
                    "pageNumber": seg["pageNumber"]
                } for seg in segment_list
            }
            final_notes[slide_key] = note_sections  

        print("ê° ìŠ¬ë¼ì´ë“œë³„ í•„ê¸° ìƒì„± ì™„ë£Œ")
        shutil.rmtree(temp_dir)
        return JSONResponse(content=final_notes)
    
    except Exception as e:
        if 'temp_dir' in locals() and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        return JSONResponse(status_code=500, content={"message": f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"})
